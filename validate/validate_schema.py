# validate_schema.py
import json
import os
import time
import logging
import sys # <-- TH√äM M·ªöI ƒê·ªÇ S·ª¨A L·ªñI WINDOWS
from collections import Counter
from datetime import datetime

from jsonschema import validate, ValidationError
# Gi·∫£ ƒë·ªãnh schema_definitions.py ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü b∆∞·ªõc 1
from schema_definitions import CORE_SCHEMA, EXTENDED_SCHEMA 

# C·∫•u h√¨nh Logging
LOG_FILE = 'validation_report.log'
logging.basicConfig(level=logging.INFO, 
                    format='%(levelname)s: %(message)s',
                    handlers=[
                        # Th√™m encoding='utf-8' cho FileHandler
                        logging.FileHandler(LOG_FILE, mode='w', encoding='utf-8'), 
                        # D√πng sys.stdout ƒë·ªÉ √©p UTF-8 tr√™n console Windows
                        logging.StreamHandler(sys.stdout) 
                    ])
logger = logging.getLogger(__name__)

# --- C√ÅC H√ÄM KI·ªÇM TRA CH√çNH ---

def validate_jsonl_file(file_path, schema, schema_name):
    """
    Th·ª±c hi·ªán ki·ªÉm tra c·∫•u tr√∫c (jsonschema) v√† logic nghi·ªáp v·ª• trong m·ªôt l·∫ßn qu√©t (One-Pass).
    """
    logger.info(f"\n--- B·∫Øt ƒë·∫ßu ki·ªÉm tra {schema_name} ({file_path}) ---")
    
    validation_summary = {
        'struct_errors': 0,
        'logic_errors': 0,
        'duplicate_ids': [],
        'post_ids': [],
        'user_ids': set()
    }
    
    if not os.path.exists(file_path):
        logger.error(f"L·ªñI: Kh√¥ng t√¨m th·∫•y file t·∫°i ƒë∆∞·ªùng d·∫´n: {file_path}")
        validation_summary['struct_errors'] += 1
        return validation_summary

    total_records = 0
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_number, line in enumerate(f): 
            if not line.strip(): continue
            total_records += 1
            
            try:
                data = json.loads(line)
                
                # 1. Ki·ªÉm tra C·∫•u tr√∫c v√† Ki·ªÉu d·ªØ li·ªáu (JSONSchema)
                validate(instance=data, schema=schema)
                
                # Thu th·∫≠p ID
                if 'id' in data: validation_summary['post_ids'].append(data['id'])
                if 'user_id' in data: validation_summary['user_ids'].add(data['user_id'])
                
                # 2. Ki·ªÉm tra Logic Nghi·ªáp v·ª•
                if schema_name == "EXTENDED_SCHEMA":
                    img_info = data.get('image_info', {})
                    text_content = data.get('clean_text', '')
                    
                    # 2.1. Ki·ªÉm tra Quy t·∫Øc C·∫•u tr√∫c c·ª• th·ªÉ (image_size ph·∫£i l√† [224, 224])
                    if img_info.get('image_size') not in ([224, 224], None):
                        logger.error(f"L·ªñI LOGIC D√≤ng {line_number + 1}: image_size kh√¥ng ph·∫£i [224, 224].")
                        validation_summary['logic_errors'] += 1
                        
                    # 2.2. Ki·ªÉm tra consistency c·ªßa video/keyframe
                    is_video = img_info.get('is_video', False)
                    keyframe_paths = img_info.get('keyframe_paths', [])
                    if is_video and not keyframe_paths:
                        logger.error(f"L·ªñI LOGIC D√≤ng {line_number + 1}: is_video=True nh∆∞ng keyframe_paths tr·ªëng.")
                        validation_summary['logic_errors'] += 1
                    elif not is_video and keyframe_paths:
                        logger.error(f"L·ªñI LOGIC D√≤ng {line_number + 1}: is_video=False nh∆∞ng keyframe_paths c√≥ d·ªØ li·ªáu.")
                        validation_summary['logic_errors'] += 1

                    # 2.3. Ki·ªÉm tra clean_text consistency (ƒê√É S·ª¨A L·ªñI LOGIC isupper - L·ªói 1)
                    
                    # Logic 1: Ki·ªÉm tra ƒë·ªô d√†i
                    is_length_valid = len(text_content) >= 5 and len(text_content) <= 5000
                    
                    # Logic 2: Ki·ªÉm tra ch·ªØ hoa (any upper case char)
                    has_uppercase = any(c.isupper() for c in text_content if c.isalpha())
                    
                    if not is_length_valid:
                        logger.error(f"L·ªñI LOGIC D√≤ng {line_number + 1}: clean_text kh√¥ng nh·∫•t qu√°n (qu√° ng·∫Øn/qu√° d√†i).")
                        validation_summary['logic_errors'] += 1
                    elif has_uppercase:
                        logger.error(f"L·ªñI LOGIC D√≤ng {line_number + 1}: clean_text ch·ª©a k√Ω t·ª± ch·ªØ hoa (ch∆∞a lowercase).")
                        validation_summary['logic_errors'] += 1
                        
                    # 2.4. Ki·ªÉm tra timestamp logic
                    current_epoch = int(time.time())
                    if data.get('timestamp') and data.get('timestamp') > current_epoch:
                        logger.error(f"L·ªñI LOGIC D√≤ng {line_number + 1}: timestamp t∆∞∆°ng lai.")
                        validation_summary['logic_errors'] += 1

                    # 2.5. Ki·ªÉm tra media_url <-> is_video 
                    if data.get('media_url') and not img_info.get('processed_path'):
                        logger.warning(f"C·∫¢NH B√ÅO D√≤ng {line_number + 1}: C√≥ media_url th√¥ nh∆∞ng thi·∫øu processed_path.")

                
            except json.JSONDecodeError:
                logger.error(f"L·ªñI C√ö PH√ÅP D√≤ng {line_number + 1}: JSON kh√¥ng h·ª£p l·ªá.")
                validation_summary['struct_errors'] += 1
            except ValidationError as e:
                logger.error(f"L·ªñI SCHEMA D√≤ng {line_number + 1} ({e.path}): {e.message}")
                validation_summary['struct_errors'] += 1
                
    # 3. Ki·ªÉm tra T√≠nh Duy Nh·∫•t (Logic C·∫•u tr√∫c cu·ªëi c√πng)
    id_counts = Counter(validation_summary['post_ids'])
    validation_summary['duplicate_ids'] = [id for id, count in id_counts.items() if count > 1]
    validation_summary['struct_errors'] += len(validation_summary['duplicate_ids'])
    
    if validation_summary['duplicate_ids']:
        logger.error(f"L·ªñI C·∫§U TR√öC: {len(validation_summary['duplicate_ids'])} ID b·ªã tr√πng l·∫∑p.")
        
    logger.info(f"Ho√†n t·∫•t ki·ªÉm tra {total_records} b·∫£n ghi. T·ªïng l·ªói: {validation_summary['struct_errors'] + validation_summary['logic_errors']}.")
    return validation_summary 

# --- H√ÄM CH·∫†Y CH√çNH V√Ä T·ªîNG H·ª¢P ---

def run_validation(raw_path: str, processed_path: str):
    """
    Run validation on specified files.
    
    Args:
        raw_path: Path to raw JSONL file (CORE_SCHEMA)
        processed_path: Path to processed JSONL file (EXTENDED_SCHEMA)
    """
    # 1. KI·ªÇM TRA ƒê·∫¶U V√ÄO (CORE SCHEMA)
    core_results = validate_jsonl_file(raw_path, CORE_SCHEMA, "CORE_SCHEMA")

    # 2. KI·ªÇM TRA ƒê·∫¶U RA (EXTENDED SCHEMA)
    extended_results = validate_jsonl_file(processed_path, EXTENDED_SCHEMA, "EXTENDED_SCHEMA")

    # 3. T·ªîNG K·∫æT L·ªñI
    total_errors = core_results['struct_errors'] + core_results['logic_errors'] + \
                   extended_results['struct_errors'] + extended_results['logic_errors']
    
    logger.info("\n" + "="*70)
    logger.info("                         ‚ú® B√ÅO C√ÅO T·ªîNG K·∫æT VALIDATION ‚ú®")
    logger.info("="*70)
    logger.info(f"T·ªîNG S·ªê L·ªñI PH√ÅT HI·ªÜN: {total_errors}")
    logger.info(f"Report chi ti·∫øt ƒë√£ ƒë∆∞·ª£c ghi v√†o file: {LOG_FILE}")
    
    logger.info("\n--- CHI TI·∫æT T√ìM T·∫ÆT ---")
    logger.info(f"CORE SCHEMA (A) - L·ªói C·∫•u tr√∫c: {core_results['struct_errors']}")
    logger.info(f"EXTENDED SCHEMA (B/C) - L·ªói C·∫•u tr√∫c: {extended_results['struct_errors']}")
    logger.info(f"EXTENDED SCHEMA (B/C) - L·ªói Logic Nghi·ªáp v·ª•: {extended_results['logic_errors']}")
    
    if total_errors > 0:
        logger.error("‚ö† H√ÄNH ƒê·ªòNG: C·∫¶N Y√äU C·∫¶U C√ÅC TH√ÄNH VI√äN S·ª¨A D·ªÆ LI·ªÜU. Vui l√≤ng xem log.")
    else:
        logger.info("üëç D·ªÆ LI·ªÜU ƒê·∫†T CHU·∫®N. C√≥ th·ªÉ ti·∫øp t·ª•c G√°n nh√£n/X√¢y d·ª±ng Graph.")
    
    return total_errors

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Validate Fakeddit data against schemas')
    parser.add_argument(
        '--raw',
        default='data/01_raw/Fakeddit/Fakeddit_pilot_processed_200.jsonl',
        help='Path to raw JSONL file (CORE_SCHEMA)'
    )
    parser.add_argument(
        '--processed',
        default='data/03_clean/Fakeddit/train.jsonl',
        help='Path to processed JSONL file (EXTENDED_SCHEMA)'
    )
    
    args = parser.parse_args()
    run_validation(args.raw, args.processed)