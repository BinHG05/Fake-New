import json
import os
import time
import sys

# ThÃªm Ä‘Æ°á»ng dáº«n Ä‘á»ƒ Python tÃ¬m tháº¥y module dÃ¹ cháº¡y tá»« root
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(CURRENT_DIR)

try:
    from reddit_crawler import RedditCrawler 
except ImportError:
    # Náº¿u khÃ´ng tÃ¬m tháº¥y, thá»­ import theo absolute path (phÃ²ng há»)
    try:
        from src.data.reddit_crawler import RedditCrawler
    except ImportError:
        print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y module reddit_crawler.py")
        sys.exit(1)

# Sá»­ dá»¥ng Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i dá»±a trÃªn vá»‹ trÃ­ file hiá»‡n táº¡i
# LÃªn 2 cáº¥p: src/data -> src -> ProjectRoot
ROOT_DIR = os.path.dirname(os.path.dirname(CURRENT_DIR))
INPUT_FILE = os.path.join(ROOT_DIR, "data", "03_clean", "Fakeddit", "labeled_master.jsonl")
OUTPUT_FILE = os.path.join(ROOT_DIR, "data", "reddit_enriched_data.jsonl")

# HÃ m kiá»ƒm tra cÃ¡c ID Ä‘Ã£ xá»­ lÃ½
def get_existing_ids(output_path):
    if not os.path.exists(output_path):
        return set()
    existing_ids = set()
    try:
        with open(output_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                try:
                    data = json.loads(line)
                    if 'id' in data:
                        existing_ids.add(data['id'])
                except:
                    continue
    except Exception as e:
        print(f"âš ï¸ Warning: Lá»—i khi Ä‘á»c file cÅ©: {e}")
    return existing_ids

def reddit_enriched_data():
    print(f"ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh Enrich Data...")
    
    # Khá»Ÿi táº¡o crawler tá»« file gá»‘c 
    crawler = RedditCrawler(debug=False)

    if not os.path.exists(INPUT_FILE):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file input: {INPUT_FILE}")
        print(f"ğŸ‘‰ HÃ£y Ä‘áº£m báº£o file 'labeled_master.jsonl' náº±m trong thÆ° má»¥c 'data' cá»§a project.")
        return

    # Äáº£m báº£o thÆ° má»¥c Ä‘áº§u ra tá»“n táº¡i
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    # Láº¥y danh sÃ¡ch ID Ä‘Ã£ hoÃ n thÃ nh Ä‘á»ƒ Resume
    done_ids = get_existing_ids(OUTPUT_FILE)
    print(f"ğŸ”„ ÄÃ£ xá»­ lÃ½ xong {len(done_ids)} bÃ i viáº¿t trÆ°á»›c Ä‘Ã³.")

    # Äá»c dá»¯ liá»‡u Ä‘Ã£ gáº¯n label
    print(f"ğŸ“– Äang Ä‘á»c dá»¯ liá»‡u tá»« {INPUT_FILE}...")
    target_posts = []
    try:
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line: continue
                try:
                    target_posts.append(json.loads(line))
                except:
                    continue
    except Exception as e:
        print(f"âŒ Lá»—i Ä‘á»c file input: {e}")
        return

    total_posts = len(target_posts)
    print(f"âœ… TÃ¬m tháº¥y {total_posts} bÃ i viáº¿t cáº§n xá»­ lÃ½.")
    
    processed_count = 0
    skipped_count = 0
    
    for index, post in enumerate(target_posts):
        post_id = post.get('id')
        
        # Safety check: Náº¿u khÃ´ng cÃ³ ID thÃ¬ bá» qua
        if not post_id:
            continue

        # Kiá»ƒm tra Resume (Náº¿u ID Ä‘Ã£ cÃ³ thÃ¬ bá» qua)
        if post_id in done_ids:
            skipped_count += 1
            # Chá»‰ in log má»—i 50 bÃ i skip Ä‘á»ƒ Ä‘á»¡ spam mÃ n hÃ¬nh
            if skipped_count % 50 == 0:
                print(f"â© ÄÃ£ bá» qua {skipped_count} bÃ i cÅ©...", end='\r')
            continue

        fake_permalink = f"/comments/{post_id}/"
        print(f"ğŸ“¥ [{index + 1}/{total_posts}] Äang láº¥y Cascade cho: {post_id}...")
        
        try:
            # Gá»i hÃ m fetch comments cÃ³ sáºµn
            cascade_data = crawler.fetch_comments(fake_permalink)
            
            # Gá»™p dá»¯ liá»‡u cascade vÃ o object gá»‘c
            post['cascade'] = cascade_data
            post['metadata_enrich'] = {
                "enriched_at": int(time.time()),
                "comment_count_fetched": len(cascade_data)
            }

            # LÆ°u ngay dá»¯ liá»‡u (mode 'a' - append)
            with open(OUTPUT_FILE, 'a', encoding='utf-8') as out_file:
                out_file.write(json.dumps(post, ensure_ascii=False) + '\n')
            
            processed_count += 1
            print(f"   âœ… OK! Láº¥y Ä‘Æ°á»£c {len(cascade_data)} comments.")

        except Exception as e:
            print(f"   âŒ Lá»—i khi xá»­ lÃ½ {post_id}: {e}")

        # Thá»i gian nghá»‰ Ä‘á»ƒ trÃ¡nh Rate Limit
        time.sleep(2.0) 

    print("\n" + "="*50)
    print(f"ğŸ XONG!")
    print(f"ğŸ“Š Tá»•ng cá»™ng: {total_posts}")
    print(f"âœ… Má»›i lÃ m xong: {processed_count}")
    print(f"â© ÄÃ£ bá» qua: {skipped_count}")
    print(f"ğŸ’¾ File káº¿t quáº£: {OUTPUT_FILE}")

if __name__ == "__main__":
    try:
        # Fix encoding cho Windows terminal
        if sys.platform.startswith('win'):
            os.system('chcp 65001')
    except:
        pass
    reddit_enriched_data()