import requests
import json
import time
from datetime import datetime
import os

# --- C·∫§U H√åNH ---
# 1. Danh s√°ch c√°c nh√≥m c·∫ßn qu√©t tin m·ªõi (Real-time)
SUBREDDITS = ["worldnews", "news", "politics", "technology", "conspiracy", "fake_news"]

# 2. T·ª± ƒë·ªông x√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n ƒë·ªÉ l∆∞u file v√†o th∆∞ m·ª•c 'data' ·ªü ngo√†i c√πng
# Logic: T·ª´ file n√†y (src/data) ƒëi ng∆∞·ª£c ra 2 c·∫•p l√† t·ªõi th∆∞ m·ª•c g·ªëc -> v√†o folder data
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__)) # Th∆∞ m·ª•c ch·ª©a file code n√†y
ROOT_DIR = os.path.dirname(os.path.dirname(CURRENT_DIR)) # Th∆∞ m·ª•c g·ªëc d·ª± √°n (FAKE-NEW)
DATA_DIR = os.path.join(ROOT_DIR, "data") # Th∆∞ m·ª•c kho ch·ª©a data

# T·∫°o t√™n file k·∫øt qu·∫£
OUTPUT_FILE = os.path.join(DATA_DIR, "reddit_realtime_data.jsonl")

def crawl_reddit_realtime():
    print(f"üöÄ B·∫ÆT ƒê·∫¶U QU√âT D·ªÆ LI·ªÜU REAL-TIME")
    print(f"üìÇ File s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_FILE}")
    print("-" * 50)
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    all_posts = []

    for group in SUBREDDITS:
        try:
            print(f"üì° ƒêang qu√©t nh√≥m: r/{group}...")
            # L·∫•y d·ªØ li·ªáu JSON c√¥ng khai (C·ª≠a sau - Kh√¥ng c·∫ßn API Key)
            url = f"https://www.reddit.com/r/{group}/new.json?limit=20"
            
            resp = requests.get(url, headers=headers, timeout=10)
            
            if resp.status_code == 200:
                data = resp.json()
                posts = data['data']['children']
                
                count = 0
                for post in posts:
                    p = post['data']
                    
                    # Map d·ªØ li·ªáu sang ƒë√∫ng Schema JSONL c·ªßa nh√≥m
                    item = {
                        "id": p['id'],
                        "timestamp": p['created_utc'],  # Th·ªùi gian th·ª±c
                        "label": "Unlabeled",           # Ch∆∞a c√≥ nh√£n
                        "raw_text": f"{p['title']} {p['selftext']}", # Ti√™u ƒë·ªÅ + N·ªôi dung
                        "media_url": p.get('url_overridden_by_dest', None), # Link ·∫£nh (n·∫øu c√≥)
                        "user_id": p['author'],         # T√°c gi·∫£
                        "source": f"r/{group}"
                    }
                    all_posts.append(item)
                    count += 1
                print(f"   ‚úÖ L·∫•y ƒë∆∞·ª£c {count} b√†i m·ªõi.")
            else:
                print(f"   ‚ö†Ô∏è L·ªói k·∫øt n·ªëi r/{group}: {resp.status_code}")
                
        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")
            
        time.sleep(1) # Ngh·ªâ 1 gi√¢y ƒë·ªÉ kh√¥ng b·ªã ch·∫∑n

    # L∆∞u file
    if all_posts:
        print("-" * 50)
        print(f"üíæ ƒêang l∆∞u {len(all_posts)} d√≤ng d·ªØ li·ªáu...")
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c data t·ªìn t·∫°i
        os.makedirs(DATA_DIR, exist_ok=True)
        
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            for post in all_posts:
                json.dump(post, f, ensure_ascii=False)
                f.write('\n')
                
        print("üéâ HO√ÄN T√ÄNH! NHI·ªÜM V·ª§ XONG.")
    else:
        print("üò≠ Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu n√†o.")

if __name__ == "__main__":
    crawl_reddit_realtime()