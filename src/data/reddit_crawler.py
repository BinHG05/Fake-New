import requests
import json
import time
import os

# --- C·∫§U H√åNH ---
SUBREDDITS = ["worldnews", "news", "politics", "technology", "conspiracy", "fake_news"]

# T·ª± ƒë·ªông x√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n l∆∞u v√†o folder 'data' ·ªü ngo√†i c√πng
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(os.path.dirname(CURRENT_DIR))
DATA_DIR = os.path.join(ROOT_DIR, "data")
OUTPUT_FILE = os.path.join(DATA_DIR, "reddit_realtime_data.jsonl")

# --- H√ÄM B·ªä THI·∫æU TRONG ƒêO·∫†N C·ª¶A B·∫†N (QUAN TR·ªåNG) ---
def get_existing_ids(file_path):
    """ƒê·ªçc file c≈© ƒë·ªÉ l·∫•y danh s√°ch ID ƒë√£ c√≥ (tr√°nh l∆∞u tr√πng)"""
    existing_ids = set()
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        record = json.loads(line)
                        if 'id' in record:
                            existing_ids.add(record['id'])
                    except:
                        continue
        except:
            pass
    return existing_ids
# ----------------------------------------------------

def crawl_reddit_final():
    print(f"üöÄ B·∫ÆT ƒê·∫¶U QU√âT D·ªÆ LI·ªÜU REAL-TIME (SCHEMA CHU·∫®N)")
    print(f"üìÇ File l∆∞u t·∫°i: {OUTPUT_FILE}")
    print("-" * 50)
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    
    # 1. Load ID c≈© ƒë·ªÉ l·ªçc tr√πng
    existing_ids = get_existing_ids(OUTPUT_FILE)
    print(f"üìä Trong kho ƒëang c√≥: {len(existing_ids)} b√†i.")
    
    buffer_to_write = []

    for group in SUBREDDITS:
        try:
            print(f"üì° ƒêang qu√©t nh√≥m: r/{group}...")
            url = f"https://www.reddit.com/r/{group}/new.json?limit=25"
            
            resp = requests.get(url, headers=headers, timeout=10)
            
            if resp.status_code == 200:
                data = resp.json()
                children = data['data']['children']
                
                count = 0
                for child in children:
                    p = child['data']
                    post_id = str(p['id'])

                    # Check tr√πng: N·∫øu ID ƒë√£ c√≥ trong kho th√¨ b·ªè qua
                    if post_id in existing_ids:
                        continue

                    # X·ª≠ l√Ω media_url: Ph·∫£i l√† String r·ªóng "" n·∫øu kh√¥ng c√≥ ·∫£nh
                    media = p.get('url_overridden_by_dest', "")
                    if media is None: 
                        media = ""

                    # --- MAP ƒê√öNG CHU·∫®N SCHEMA ---
                    item = {
                        "id": post_id,
                        "timestamp": int(p['created_utc']),
                        "label": "Unlabeled",
                        "raw_text": f"{p['title']} {p['selftext']}".strip(),
                        "media_url": str(media),
                        "user_id": str(p['author']),
                        "retweet_count": int(p.get('num_comments', 0))
                    }

                    buffer_to_write.append(item)
                    existing_ids.add(post_id) 
                    count += 1
                
                print(f"   ‚úÖ L·∫•y ƒë∆∞·ª£c {count} b√†i m·ªõi.")
            else:
                print(f"   ‚ö†Ô∏è L·ªói k·∫øt n·ªëi r/{group}: {resp.status_code}")
                
        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")
            
        time.sleep(2) # Ngh·ªâ 2s tr√°nh ch·∫∑n IP

    # 2. L∆∞u xu·ªëng file (Mode 'a' - Append ƒë·ªÉ c·ªông d·ªìn)
    if buffer_to_write:
        print("-" * 50)
        
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c data t·ªìn t·∫°i
        os.makedirs(DATA_DIR, exist_ok=True)
        
        with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:
            for post in buffer_to_write:
                json.dump(post, f, ensure_ascii=False)
                f.write('\n')
                
        print(f"üéâ TH√ÄNH C√îNG! ƒê√£ l∆∞u th√™m {len(buffer_to_write)} b√†i vi·∫øt m·ªõi.")
    else:
        print("üò¥ Kh√¥ng c√≥ b√†i m·ªõi n√†o so v·ªõi l·∫ßn ch·∫°y tr∆∞·ªõc.")

if __name__ == "__main__":
    crawl_reddit_final()